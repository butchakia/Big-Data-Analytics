{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a274fbd5-f2ee-4b5d-b326-76bd6f3a4f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DSCI 617 â€“ Homework 08\n",
    "**Jeffery Boczkaja**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10de6856-8f0d-4cd9-aa54-4cc3859b58a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cf5161-6a83-4c46-a8ff-2fd029085df3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Problem 1: Creating Streaming DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7171f7dd-8810-4b91-9284-019384be6378",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will start by loading a single file into a static DataFrame, and then using this DataFrame to define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427281d9-05f3-483e-a1fd-4dec64afc22f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the DataFrame:\nStructType([StructField('step', IntegerType(), True), StructField('type', StringType(), True), StructField('amount', DoubleType(), True), StructField('nameOrig', StringType(), True), StructField('oldbalanceOrg', DoubleType(), True), StructField('newbalanceOrig', DoubleType(), True), StructField('nameDest', StringType(), True), StructField('oldbalanceDest', DoubleType(), True), StructField('newbalanceDest', DoubleType(), True)])\nFirst 5 rows of the DataFrame:\n+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|\n|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|\n|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|\n|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|\n|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|\n+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/paysim/step_001.csv\", header=True, inferSchema=True)\n",
    "schema = df.schema\n",
    "print(\"Schema of the DataFrame:\")\n",
    "print(schema)\n",
    "\n",
    "print(\"First 5 rows of the DataFrame:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f1cf9b-be43-4212-a971-bb89a8809d14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will now create a streaming DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873326d4-073a-4573-b742-fee05a83445f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paysim_stream = spark.readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/FileStore/tables/paysim/\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33e1fb9-2c5f-4931-b515-873c8c5a3bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will verify if the schema is streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d288b379-ca7f-49c3-be3f-1f00cb0a7647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the DataFrame streaming? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is the DataFrame streaming?\", paysim_stream.isStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99913ab3-722e-45c1-b2f3-c43a5b0202af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Problem 2: Apply Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d62a1f-0f08-4335-9d6f-6252a9960f33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will apply two different sets of transformations to the PaySim stream. We will start by grouping the results\n",
    "according to the transaction type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea336a1-6f7f-443c-97ec-7639e54eaac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type_summary = paysim_stream.groupBy(\"type\") \\\n",
    "    .agg(\n",
    "        expr(\"count(*) as n\"),\n",
    "        expr(\"round(avg(amount), 2) as avg_amount\"),\n",
    "        expr(\"min(amount) as min_amount\"),\n",
    "        expr(\"max(amount) as max_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"n\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d88f3c-8c58-45f2-9605-c62f17e0f325",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will verify the DataFrame is streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedde12e-2837-4460-b481-bdd83bedd413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the DataFrame streaming? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is the DataFrame streaming?\", type_summary.isStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ba6f2a-ef1c-45d9-b716-8567e9c2d3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we will group the transfers according to their destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80bcb94c-5d9e-46ef-be85-aece574aaec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "destinations = paysim_stream.filter(col(\"type\") == \"TRANSFER\") \\\n",
    "    .groupBy(\"nameDest\") \\\n",
    "    .agg(\n",
    "        expr(\"count(*) as n\"),\n",
    "        expr(\"sum(amount) as total_amount\"),\n",
    "        expr(\"round(avg(amount), 2) as avg_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"n\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88af82a8-5780-42a3-bcb2-d714175c403b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will verify the DataFrame is streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8891c373-3aae-4242-adf9-71744ecefd23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the DataFrame streaming? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is the DataFrame streaming?\", destinations.isStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e5c793-71d9-44a2-a718-808c18d540e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Problem 3: Define Output Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3120f09-c0e7-4b62-b972-f7fcd8d889f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will now define in-memory output sinks for both of the transformed DataFrames we created in Problem 2.\n",
    "We will do this by creating a DataStreamWriter object for each of these DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c32d9ff-bcba-4775-8d9a-bd59118af498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type_summary_writer = type_summary.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"type_summary\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "destinations_writer = destinations.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"destinations\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36df3724-1293-4af4-8e61-3d5f23443040",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Problem 4: Start and Monitor the Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52ecb39-95c0-4484-a1a1-3afef96f46d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will now start the two data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f6158c-9f02-4c73-a83b-61a171952592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is type_query active? True\nIs dest_query active? True\n"
     ]
    }
   ],
   "source": [
    "type_summary_writer_new = type_summary.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"type_summary_new\") \\\n",
    "    .format(\"memory\")\n",
    "\n",
    "type_query = type_summary_writer_new.start()\n",
    "\n",
    "destinations_writer_new = destinations.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"destinations_new\") \\\n",
    "    .format(\"memory\")\n",
    "\n",
    "dest_query = destinations_writer_new.start()\n",
    "\n",
    "print(\"Is type_query active?\", type_query.isActive)\n",
    "print(\"Is dest_query active?\", dest_query.isActive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eb46dbe-2df1-4f08-a7ac-fe81cdadab61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will now view the contents of the type_summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55032e60-ede1-4e79-940e-bf73113e1951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 5\n+--------+------+----------+----------+----------+\n|    type|     n|avg_amount|min_amount|max_amount|\n+--------+------+----------+----------+----------+\n|CASH_OUT|283602| 187343.61|      0.37|     1.0E7|\n| PAYMENT|262254|  11618.97|       0.1| 115264.68|\n| CASH_IN|172327| 172399.61|      5.44|1781905.26|\n|TRANSFER| 65121|  665790.8|       2.6|     1.0E7|\n|   DEBIT|  5365|   5846.53|      0.87| 218148.28|\n+--------+------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "static_type_summary = spark.sql(\"SELECT * FROM type_summary_new\")\n",
    "record_count = static_type_summary.count()\n",
    "print(f\"Number of records: {record_count}\")\n",
    "\n",
    "static_type_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b25098c-1b38-47cc-a950-364ddbf19855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will count the unique destinations of transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f7eaa4-b329-482a-8844-63485aaf7dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|unique_dest_count|\n+-----------------+\n|            25752|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "unique_dest_count_query = \"\"\"\n",
    "SELECT COUNT(DISTINCT nameDest) as unique_dest_count\n",
    "FROM destinations_new\n",
    "\"\"\"\n",
    "\n",
    "unique_dest_count_df = spark.sql(unique_dest_count_query)\n",
    "\n",
    "unique_dest_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1472e340-e015-429a-9938-698f31b4579d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we will view the contents of the destinations table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4b1862-154c-4a49-abf8-fd7eb857969e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 184\n+-----------+---+--------------------+----------+\n|   nameDest|  n|        total_amount|avg_amount|\n+-----------+---+--------------------+----------+\n|C1590550415| 12|1.7533855849999998E7|1461154.65|\n| C306206744| 10|   7134909.749999999| 713490.97|\n| C453211571|  8|   4764787.380000001| 595598.42|\n| C564160838|  8|          3070298.59| 383787.32|\n| C248609774|  8|          6160533.15| 770066.64|\n| C985934102|  7|  3718139.3599999994| 531162.77|\n|C1916720513|  7|           4312182.8| 616026.11|\n| C451111351|  7|           5617336.9|  802476.7|\n|C1286084959|  7|          2244985.66| 320712.24|\n| C392292416|  7|           4587850.6| 655407.23|\n|C1899073220|  7|          4089296.77| 584185.25|\n|C1782113663|  7|          4161329.67| 594475.67|\n| C665576141|  6|   7626856.100000001|1271142.68|\n|C1789550256|  6|   4334877.899999999| 722479.65|\n| C998351292|  6|  1880762.2400000002| 313460.37|\n|C1170794006|  5|  3501781.2300000004| 700356.25|\n+-----------+---+--------------------+----------+\nonly showing top 16 rows\n\n"
     ]
    }
   ],
   "source": [
    "static_destinations = spark.sql(\"SELECT * FROM destinations_new\")\n",
    "record_count_dest = static_destinations.count()\n",
    "print(f\"Number of records: {record_count_dest}\")\n",
    "\n",
    "static_destinations.show(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b070e3c1-a650-48ea-acc8-6e5e2394832c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will stop the streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f59c95-76c3-424a-9e31-e71db6f5dbfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is type_query active? False\nIs dest_query active? False\n"
     ]
    }
   ],
   "source": [
    "type_query.stop()\n",
    "dest_query.stop()\n",
    "print(\"Is type_query active?\", type_query.isActive)\n",
    "print(\"Is dest_query active?\", dest_query.isActive)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Boczkaja_Jeffery_HW08",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
